---
title: "Lecture 4: Linear Regression: Probabilistic Interpretation"
output:
  prettydoc::html_pretty:
    theme: leonids
highlight: github
---
  
```{r, include=FALSE}
library(tidyverse)
library(ggthemes)
library(prettydoc)
```

## Last Time

Recall what we've done last time: 

### Linear regression

We have established the problem of *linear regression*:

* Have an unknown function $f(x): X = \mathbb{R} \rightarrow \mathbb{R}$ that we are seeking to approximate. 
* We make the assumption that the approximation function ("hypothesis")  $\tilde{f}$ is a linear function, that is 

$$\tilde{f}(x) = \beta \cdot (1, x^1, x^2, \dots, x^m)$$
for all $x = (x^1, x^2, \dots, x^m) \in X$ and some coefficient vector $\beta \in \mathbb{R}^{m+1}$. 
* We are given a training set $\{(x_i, y_i = f(x_i))\}$ that we are allowed to use in order to get a sense of what the values of $f(x)$ look like. 

* In order to determine our notion of "best approximation", we introduced the "ordinary least squares" cost function, which is a function $J(\beta): \mathbb{R}^{m+1} \rightarrow \mathbb{R}$ that measures the "cost" or "loss" associated to a given linear function represented by $\beta \in \mathbb{R}^{m+1}$. 
* By using some calculus, we saw that gradient descent could be applied to finding local minima of $J$, which would allow us to find a "good" $\beta$ hypothesis. 

## Questions

We are now left with some questions, given that we have a potential procedure for finding local minima of the cost function.

1. Is this minimum unique? If we find a $\beta$ that (locally) minimizes $J$, is this unique? Are there other linear hypotheses with equal or lower cost functions (global minima)?
2. How good of a $\beta$ is it, in terms of the prediction task we are trying to accomplish?
   * We might want to compare various hypotheses for a given regression task, or across different regression tasks. 
   * For example, suppose we had a hypothesis that predicted housing price as a function of square footage in homes. How do we know that it is doing a good job independent of it having low loss? It might be the best linear hypothesis, but still bad. How do we know this? 
   * As another example, we might have hypotheses that predict housing price and cancer tumor size. How do we say that we are generally better at 
3. Is there a better way of doing this than using gradient descent? 


## Answers

1. Sometimes no! In the case where there are *colinear* predictors (features), the global minimum of the cost function is not unique, and so it is definitely the case that the local minima guaranteed by gradient descent are not unique. We'll talk about this colinearity problem later. 
2. There are many ways of determining "goodness-of-fit", which we will talk about later. These involve: 
  * Checking how well the hypothesis performs on a "test set", which was not used in the fitting process;
  * A class of metrics that relate to the probabilistic description of linear regression:
      * Checking "p-values" for coefficients;
      * Checking that a "normality assumption" holds;
  * Checking for outliers in the training set or points that "contributed unduly" to the model
  * Information-theoretic criteria for measuring goodness-of-fit that tell us "how much information we get from using the model"
3. Yes! There are ways to get closed-form solutions that just involve matrix algebra. We'll see how to do this shortly. 

## Probability Theory

We are going to address #2 in more detail first, which will allow us to get a sense of how well our regressions are performing. This requires introducing the probabilistic interpretation of linear regression, which requires us to step back and discuss some basic probability theory. 

### The Rules of Probability

**Definition**: A *probability space* is a set $\Omega$ with a *probability function* $$p: \{ \text{subsets of } \Omega \} \rightarrow \mathbb{R}$$ that satisfies the following properties:

1. $p(\Omega) = 1$ (law of total probability)
2. $p(\emptyset) = 0$ (probability of an empty event is 0)
3. $p(\bigcup_i X_i) = \sum_i p(X_i)$ for any countable collection of *disjoint* subsets $X_i$ of $\Omega$. 

The subsets of $\Omega$ are called *events* or *outcomes*, and we speak of $p(X)$ as "the probability of the event $X$", for $X \subseteq \Omega$. 

#### Example

The most well-cited example is the probability space associated to flipping a fair coin. In this case, we have that $\Omega = \{ \text{set of all possible flips of a single coin} \} = \{ \emptyset, H, T, H \vee T\} \}.$

The probability function associated to this is:

$$
\begin{equation}
p(\{ H\}) = .5 \\
p(\{T\}) = .5 \\
p(\emptyset) = 0 \\
p(\Omega = H \vee T) = 1
\end{equation}
$$
It is often important to talk about *conditional* probability of events. 

**Definition** Let $X,Y$ be two events in $\Omega$. Then the *conditional probability of $X$ given $Y$* is given by 
$$ p(X|Y) := \frac{P(X \cap Y)}{P(Y)} = \frac{P(X \text{ and } Y)}{P(Y)}$$
Two events are called *independent* if $P(X \text{ and } Y) = P(X)*P(Y)$, in which case the above formula reduces to
$$P(X|Y) = P(X)$$, which means that "$Y$ does not effect the probability of $X$ happening".

**Example:** Let $\Omega$ represent the space of all rolls of two die. Let $Y$ be the event that the first roll is a 1, and $X$ be the event that the 2nd roll is a 1. Then $X$, $Y$ are independent, and 

**Properties:**

* If $X \subseteq Y$, $p(Y|X) = 1$.
* If $X \subseteq Y$, $p(X|Y) = p(X) / p(Y)$
* If $X, Y$ are disjoint (mutually exclusive), then $p(X | Y) = 0$. 
* $P(X \cap Y) = P(Y)(X|Y) = P(X)(Y|X)$

### Random Variables

**Definition**: A *random variable* is a real valued function on a probability space $S : \Omega \rightarrow \mathbb{R}$.

**Example**: Let $\Omega$ be the space of single coin flips like we saw before. Suppose that someone gives you \$50 for a tails and \$20 for a heads. Then this defines a random variable $S$, specified by two values: 

$$
\begin{equation}
S(H) = 20 \\
S(T) = 50
\end{equation}
$$


#### Distributions

**Definition**: The *distribution of a random variable* $S$ defined on $(\Omega, p_\Omega)$ is the probability space with underlying set $\mathbb{R}$ defined by the values of the random variable. That is, the probability associated to any subset  $X \subset \mathbb{R}$ is 
$$p_S (X) = p_\Omega (\omega \in \Omega \text{ with } S(\omega) \in X) = p_\Omega (S^{-1}(X)) .$$

**Example**: In the previous example, we have that the distribution for $S$ is given by 
$$p_S(X) =
\begin{cases}
.5 & \text{one of 20,50 in } X  \\
0 & \text{neither 20, 50 in } X \\
1 & \text{both 20, 50 in } X \\
\end{cases}
$$

#### Density Functions

Sometimes random variables have *density functions* associated to them, which allow us to easily describe the distributions associated to the random variables.

**Definition:** Let $S$ be a random variable. Then $S$ has a *density function* $s: \mathbb{R} \rightarrow \mathbb{R}$ if 

$$ P_S (a \leq X \leq b) = \int_a^b s(x) dx.$$

We can think of $s(x)$ as being the probability that $S$ falls in a small interval $[x, x+ dx]$.



#### Expectations

**Definition**: Let $\Omega$ be a probability space and let $S$ be a random variable defined on $\Omega$. Then the **expected value** is defined as the sum

$$\mathbb{E}(S) = \sum_{\omega \in \Omega} p(\omega) \cdot S(\omega) = \int_{\Omega} S(\omega) d\omega.$$

This is the "expectation" or "average value" of $S$ over $\Omega$. We take the weighted average of the values of $\Omega$, where the weights come from the probability of obtaining the value.

**Properties:**

1. The expectation is linear. If we have two random variables, $S_1, S_2$, and constants $a_1, a_2$ then

$$ \mathbb{E}(a_1 * S_1 + a_2 * S_2) = \ a_1 \mathbb{E}(S_1) + a_2 \mathbb{E}(S_2) $$

(just think of a normal integral).

2. If $S$ has a density function $s$, then 
$$\mathbb{E}(S) = \int_{\mathbb{R}} s(x) *x \; dx.$$

**Example**: 

In the coin-toss example before with $S(H) = 20, S(T) = 50$, then 

$$\mathbb{E}(S) = p(H) * s(H) + p(T) * s(T) = (.5)(20) + (.5)(50) = 10 + 25 = 35$$ 

which amounts to a $35 average payout. 

### The Normal Distribution

The normal distribution is a particularly important (class of) distribution(s), which you almost certainly have seen before. It is often called a "bell curve", and the plot of its density function looks something like:

```{r normal_dist, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
data.frame(x=rnorm(100000)) %>% ggplot(aes(x=x)) + geom_density() + theme_tufte(ticks = FALSE) + geom_vline(xintercept = 0, color="blue") +  geom_vline(xintercept = 1, color="red") + geom_vline(xintercept =-1, color="red") + xlab("") + ylab("") + ggtitle("The Normal Distribution") + annotate(geom ="label", label="+1 sd", x=1, -.05) + annotate(geom ="label", label="-1 sd", x=-1, -.05) + annotate(geom="label", label="mean", x=0,y=-.05)
```

**Facts**:

1. A normal distribution is determined uniquely by two parameters:

* The **mean** $\mu$, which is the "center" of the density function;
* The **standard deviation** $\sigma$, which defines the "spread" of the normal distribution. This has the property that about 68% of the mass of the normal distribution lies in the interval $(\mu - \sigma, \mu + \sigma)$ around the mean. 

We refer to the associated normal distribution as $N(\mu, \sigma)$.

2. The normal distribution shows up in nature, mostly due to the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). Roughly speaking, if we sample a large number of observations of some identical natural phenomenon in such a way that the measurements don't depend on one-another, then the average (over many repetitions of this measurement process) will be normally distributed. 

For our purposes, the thing to keep in mind is systematic error associated to a sampling process -- in our regression setup, we might imagine that the training data comes from a "real function $f$" with some associated measurement error. Assuming that repeated measurements of $f(x)$ for a given value of $x$ are indepdendent, we might assume that the errors that we observe in the training set are normally distributed. We'll talk about this more shortly. 

3. The density function associated to the standard normal distribution $N(0, 1)$ is
$$ \phi(x) = \frac{e^{-x^2/2}}{\sqrt{2\pi}}$$

The $\sqrt{2\pi}$ factor is a normalization so that the total integral is 1, and the $1/2$ factor in the exponent ensures that the standard deviation is 1. The general normal distribution $N(\mu, \sigma)$ has a density function that is obtained by translating and scaling:

$$ \phi_{\mu, \sigma} = \frac{1}{\sigma} \phi\left(\frac{x-\mu}{\sigma}\right).$$

## Linear Regression

With all that probability theory in hand, we can now discuss the probabilistic model for linear regression.

### Model Assumptions
There are several basic assumptions of the model:

1. Let $x_i \in X$ be a training point in the feature space $x$. We assume that the observed value $y_i$ is given by our linear hypothesis $\beta$ with some noise:

$$y_i = \beta \cdot x_i + \epsilon_i $$, where $\epsilon_i$ is an error term that is sampled from some distribution. If we hypothetically made many measurements of the value of $f(x_i) = y_i$, then we would see variation due to sampling from the error distribution that $\epsilon_i$ comes from. 

2. The error terms $\epsilon_i$ are **independent** from one-another, i.e. the events $\{ \epsilon_i = a_i\}$ and $\{\epsilon_j = a_j\}$ are independent for all $i,j$ and potential error values $a_i$, $a_j$. 

3. The error terms $\epsilon_i$ are **identically distributed**, meaning that the underlying error distribution is the same for all $\epsilon_i$. 

4. The distribution that underlies the $\epsilon_i$ is a normal distribution $N(0, \sigma)$, where $\sigma$ is a fixed variance parameter that is determined from the training set.

We can picture the model as there being a small normal distribution that follows a line, and our observed training set is sampled from that line:
```{r echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
library(ggplot2)
library(dplyr)
library(ggthemes)
sp <- sapply(1:100, function(x) x*2 + 5)
err <- rnorm(100, sd = 10)
data.frame(y=sp+err, x=1:100) %>% ggplot(aes(x=x,y=y)) + geom_point() + geom_abline(slope = 2, intercept=5, color="red") + theme_tufte() 
```

### Likelihood

Now that we have a model for what a training set generated from a linear hypothesis should look like, we can try and model how *likely* our data is for a given hypothesis.

Remember, we assumed that the $\epsilon_i$ are distributed as a normal distribution:

$p(\epsilon_i) = \frac{e^{-\frac{\epsilon_i^2}{2\sigma^2} }}{\sqrt{\left(2\sigma \pi \right)}}$

Because we assumed that $y_i = \beta \cdot x_i + \epsilon_i$, we can write $y_i - \beta \cdot x_i = \epsilon_i$, so this becomes 

$$p(\epsilon_i) = \frac{e^{-\frac{\epsilon_i^2}{2\sigma^2} }}{\sqrt{\left(2\sigma \pi \right)}}$$

which we think of as a function of $\beta$.

Because all of the $\epsilon_i$ are assumed to be independent of one-another, we can write the probability (or *likelihood*) of observing the training set (given $\beta$) as 
$$
l(\beta) := \prod_i p(y_i; \beta) = \prod_i \frac{e^{-\frac{(y_i - \beta \cdot x_i)^2}{2\sigma^2} }}{\sqrt{\left(2\sigma \pi \right)}}
$$

This leads us to the idea of instead finding the $\beta$ that **maximizes** the likelihood, or looking for 

$$\tilde{\beta} = \textrm{argmax}_\beta \left(l(\beta)\right)$$.

Because the logarithm is a monotonic function, we can also maximize the *log-likelihood*, which turns the product into a nice sum and kills the exponent:

$$ \log(l(\beta)) = - c *\sum_i \frac{(y_i - \beta \cdot x_i)^2}{2\sigma^2} $$
The constants are independent of $\beta$, so maximizing this is the same as maximizing 

$$\sum_i -  (y_i - \beta \cdot x_i)^2$$

Which is the same as minimizing 

$$\sum_i (y_i - \beta \cdot x_i)^2.$$

This should look familiar -- this is the squared loss function that we came up with earlier. 


