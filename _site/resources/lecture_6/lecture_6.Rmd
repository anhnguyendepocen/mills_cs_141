---
title: "Lecture 6"
output: html_notebook
---

# Last Time

Last time, we discussed the probabilistic model for linear regression, where we assume that 

$$y_i = \beta \cdot x + \epsilon_i$$

for some $\beta \in \mathbb{R}^m$ and $\epsilon_i$ that are sampled from independent, identical normal distributions $N(0, \sigma)$.

Under these assumptions, we showed that the maximum liklihood estimate (MLE) of the data that came from trying to maximize the liklihood function (the joint density of the $y_i$ given a choice of $\beta$) over $\beta$ leads to the same optimization problem as trying to minimize the least squares loss function. 

The advantage of this alternative description is that we are making the underlying probabilistic assumptions of the regression model explicit -- we are making assumptions about the sampling error $\epsilon_i$ that we can hope to check in addition to the more obvious assumption that the underlying function we are approximating is linear. 

In addition, we worked through the process of actually fitting these models in R, using the formula interface and the `lm` function. 

# Goodness of Fit

We have now described several ways we can go about finding an optimal $\beta$ for linear regression, and an easy way to start fitting linear hypotheses in R. How do we go about measuring the quality of our hypotheses? 

* We might have several linear hypotheses that use various features (or combinations, transformations, etc.). How do we know which ones are better?
* Once we decide on a final hypothesis, how do we gauge its performance at the underlying prediction task that we initially were setting out to accomplish? Our goal, remember, was to approximate a function 

$$ f(x) \approx \beta \cdot x  $$
for an appropriate choice of $\beta$, given some training data $\{ (x_i, y_i)\}$. If we start plugging in new $x$ values that were not in the training set, how well will this optimal $\beta$ do? 

## Correlation and Variance

Understanding how much variability in our data is generally important -- if our data does not vary a lot, then we might hope to fit simple models to it, as it appears more homogeneous. In addition, we might want to consider how different features vary with one-another, for the purpose of choosing features or for generating a narrative around the predictive models that we end up using. 

There are several ways of going about defining "the variation of a data set", but a simple one starts off with the **covariance**, which is a simple measurement of how two different features vary with one-another.

**Definition**: Let $X,Y$ be two random variables on a probability space $\Omega$. Then the *covariance* of $X,Y$ is defined by 

$$cov(X,Y) = \mathbb{E}\left[(X - \mathbb{E}(X))(Y-\mathbb{E}(Y))\right] = \sum_{\omega \in \Omega} p(\omega)(X(\omega) - \bar{X})(Y(\omega) - \bar{Y})$$
This measures how often both $X$ and $Y$ are above/below their mean. If $X$ and $Y$ are always simultaneously in the "same direction" from their mean, then this quantity is positive. If they are in "opposite directions", then this is negative, and this gets close to zero if $X,Y$ are approximately at the same scale and do not show any pattern in when they are above/below their means. 

An important special case of this is the variance, which measures how much a random variable varies overall.

**Definition** The *variance* of a random variable $X$ is 

$$var(X) = cov(X, X) = \mathbb{E}\left[(X - \mathbb{E}(X))^2 \right] = \sum_{\omega \in \Omega} p(\omega)(X(\omega) - \bar{X})^2$$

If $X$ is generally close to its mean, then this quantity is small. If $X$ has large variations from its mean, then this quantity is large:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
a <- data.frame(x=1:100, y = sin(1:100*(pi/32)), color="red")
b <- data.frame(x=1:100, y = 10*sin(1:100*(pi/32)), color="blue")
qplot(x=x,y=y, color=color, data=rbind(a,b), geom="smooth", se=FALSE)
```

In the above, the red line exhibits smaller variance than the blue line.

A commonly-associated quantity is the *standard deviation*, which is just given by 

$$\sigma(X) = \sqrt{var(X)}.$$
**Exercise**: Calculate the variance of the Poisson distribution, which is a probability distribution on the non-negative integers, and whose probability mass function is given by 
$$ P(k) = \frac{\lambda^k} {k!} e^{-\lambda x}$$
for a parameter $\lambda$. 

Finally, we can look at the correlation, which normalizes the covariance:

**Definition**: The *correlation* of two random variables $X,Y$ is given by the normalized covariance:

$$corr(X,Y) = \frac{cov(X,Y)}{\sigma(X) \sigma(Y)}$$

This has a geometric interpretation, for $X,Y$ that have been centered to have mean 0 and live on a finite probability space: 

$$ corr(X,Y) = \frac{X \cdot Y}{\Vert X \Vert \Vert Y \Vert } = \cos(X,Y)$$

the angle between the "feature vectors" $X,Y$. 

The correlation is always between -1 and 1 (obvious from the angle interpretation). Random variables for which the correlation is positive are called *correlated*, negatively correlated variables are called *anticorrelated*, and random variables with 0 (or small) correlation are *uncorrelated*.

Because of how the covariance was defined, this measures linear relationships between $X,Y$. 

## $R^2$ 

One way of looking at the performance of the linear hypothesis is to calculate the so-called $R^2$ value, which is called the *coefficient of determination*. This is defined as 

$$R^2 = 1 - \frac{\text{Variation in target remaining after regression}}{\text{Total variation in target}}$$

If $R^2$ = 0, then none of the variation in the target is explained by the regression. If it is 1, then all of the variation in the target is explained by the regression. We make this expression more precise. The total variation in the data can be described in terms of 3 sums of squares:

* The *total sum of squares*

$$SS_{tot} = \sum_i (y_i - \bar{y})^2$$

* The *explained sum of squares* or *regression sum of squares*

$$SS_{reg} = \sum_i (\beta \cdot x_i - \bar{y})^2$$

* The *residual sum of squares*

$$SS_{res} = \sum_i (y_i - \beta \cdot x_i)^2 = \sum_i \epsilon_i ^2$$.

A fact:

For linear regression, it is always the case that 

$$SS_{tot} = SS_{reg} + SS_{res}$$.

Then we define:

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

Because of the previous equation, this is always between 0 and 1 (for linear regression), and if it is 0, this means that all of the variation comes from the residuals and none is explained by the regression. If it is 1, then all of the variation is explained by the regression.

In `summary()` applied to the output of `lm`, we get a report of the $R^2$ as well as an "adjusted" version of the $R^2$ which tries to penalize the number of explanatory features used to fit the model. 

*Exercise*: Look this up. 


# Overfitting

**Overfitting** is one of the reasons that it generally does not suffice to observe performance of our hypothesis in terms of the training set. 

Here's a great example, taken from [SE](http://stats.stackexchange.com/questions/128616/whats-a-real-world-example-of-overfitting). I'll make 99 random features to form 100 samples, with another random feature that I use as my target for a regression.

```{r}
set.seed(123)
k <- 100
data <- replicate(k, rnorm(100))
colnames(data) <- make.names(1:k)
data <- as.data.frame(data)
head(data)
```

Now I'll fit a model to predict the first random feature from the remaining. 

```{r}
fit <- lm(X1 ~ ., data=data)
summary(fit)
```
We'll get back to this, but R is being nice and actually warning us that there are 0 degrees of freedom. However:

```{r}
sum(abs(data$X1-fitted(fit)))
```

There is no error at all! The loss function is 0, and our linear hypothesis predicts the output perfectly. Obviously it is the case that there is not a real linear relationship between a bunch of random features and another random feature -- the problem is that there are only 100 sample points and 99 different features. By setting the coefficients of the linear hypothesis *just right*, we can force a linear model to fit this training set, but that is clearly describing patterns that are not there.

The summary is: *If you look hard enough, you will find patterns in anything.* How do we make sure that we are not finding spurious patterns with our linear hypotheses?

### Feature Selection
This speaks to the problem of *feature selection*. In the example above, we were simply using too many features, and our hypothesis ended up being far too complicated for the actual data we were giving it. When we are fitting a linear regression from a given set of training data, we actually have many different classes of hypotheses at our disposal. We saw this when we looked at the `iris` dataset:

```{r}
head(iris)
```

If we are looking to predict `Petal.Width`, there are actually 4 other features that we may or may not use, and our formula interface allows us to combine them in an infinitude of ways:

```{r}
lm1 <- lm(Petal.Width ~ ., data=iris)
lm2 <- lm(Petal.Width ~ . - Sepal.Width, data=iris)
lm3 <- lm(Petal.Width ~ log(Sepal.Length)*Petal.Length + Sepal.Width, data=iris)
```

How do we know which feature space we ought to be allowing our fitting process access to? It cannot be unrestricted, for two reasons:

1. There are too many combinations to try them all to figure out which one is best;
2. We can make our linear models fit our data perfectly, always. This can be done in the way described above (by generating lots of random junk features) or by using polynomial regression. Given a set of features $X_1, X_2, \dots$ we can form new features that are polynomials of those features of arbitrary degree, making a new feature space that has features like $X_1, X_1^2, X_1^3, \dots, X_2, X_2^2, X_2^3, \dots$. 

Because polynomials can be made to fit an arbitrary finite set of points, we can *always* do a polynomial regression that fits the data perfectly if we use a high enough degree.

This means that we need a way of comparing many hypotheses for the same task -- if we want to use linear models, we have to choose amongst the possible choices of feature spaces that we input as the training set into the least squares optimization problem. However, because of overfitting, we can't just measure the loss over the training set -- some overcomplicated models will have 0 loss on the training set, and those might fail miserably at generally approximating the function we are interested in approximating.

### Test Sets

One thing to do is to check the performance of the hypothesis on data that was not used to fit the model. This is really what we're after -- we're not so interested in the values of our hypothesis on the training set, per se, but we are interested in using the hypothesis to predict values of the underlying approximated function on *new data*. 

To that end, if we are given a separate *test set* of data $\{(x_i, y_i)\}$, we can evaluate the performance of a given hypothesis on the test set. This is a simulation of presenting unseen data to the hypothesis and asking it to predict values that we know. 

Unfortunately, we are not usually given a separate testing set, and so we are forced to create some sort of testing strategy that involves generating one (or many) for ourselves. There are a variety of approaches for doing this, the most basic is to reserve a **holdout set** of the training set that is not used in the training process. This holdout set is usually taken at random from the training set (so as to not bias it) and then the loss is calculated on this holdout set. Another strategy is *cross-validation*, which we will discuss later. 

**Definition**: A given hypothesis exhibits *overfitting* if:

* there is a small error on the training set
* there are large errors on a large, representative test set
* a less flexible (less complicated) model can exhibit better error on the test set.



